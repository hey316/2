{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00e45b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pHUq1J4qKFUB",
    "outputId": "7be91f90-b4ed-49e1-84d5-5c2b14ba497e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\\na. Data preparation\\nb. Generate training data\\nc. Train model\\nd. Output'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "a. Data preparation\n",
    "b. Generate training data\n",
    "c. Train model\n",
    "d. Output'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb668f2",
   "metadata": {
    "id": "6deebe40"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6e251c",
   "metadata": {
    "id": "7501f873"
   },
   "outputs": [],
   "source": [
    "#Data Prepration\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f5168e",
   "metadata": {
    "id": "c2cf4399"
   },
   "outputs": [],
   "source": [
    "sentences = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48989c73",
   "metadata": {
    "id": "f68d3e9c"
   },
   "outputs": [],
   "source": [
    "# Clean Data\n",
    "\n",
    "# remove special characters\n",
    "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
    "\n",
    "# remove 1 letter words\n",
    "sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
    "\n",
    "# lower all characters\n",
    "sentences = sentences.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5aa584",
   "metadata": {
    "id": "45a8cb65"
   },
   "outputs": [],
   "source": [
    "#Vocabulary\n",
    "\n",
    "words = sentences.split()\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4cff9",
   "metadata": {
    "id": "f37572dc"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 10\n",
    "context_size = 2\n",
    "\n",
    "#vocab_size: This is the size of your vocabulary, i.e., the total number of unique tokens (words or characters) in your dataset. You've set it to len(vocab), where vocab is likely a list or set of unique words.\n",
    "\n",
    "#embed_dim: This is the dimensionality of the embedding space. You've set it to 10, meaning each word in your vocabulary will be represented as a 10-dimensional vector. This is a hyperparameter that can be adjusted depending on the complexity of your model and dataset.\n",
    "\n",
    "#context_size: This refers to the size of the context window when training the model. You've set it to 2, which means each word will be predicted based on the surrounding 2 words (1 word to the left and 1 word to the right, in a typical setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c017635",
   "metadata": {
    "id": "ef6d5172"
   },
   "outputs": [],
   "source": [
    "#Implementation\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "#word_to_ix: This dictionary maps each word in your vocabulary (vocab) to a unique integer index. \n",
    "# The enumerate(vocab) function gives an index (i) for each word in the vocabulary, and you're using that index as the key and the word as the value.\n",
    "\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "#ix_to_word: This dictionary is the inverse of word_to_ix. It maps each index back to the corresponding word.\n",
    "# This is useful when you need to convert a predicted index back into a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471ea4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25d063d0",
    "outputId": "1d7f905c-238a-4885-d807-ad3f7820fc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['we', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'computational'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "# Data bags\n",
    "\n",
    "# data - [(context), target]\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "#Data Structure: You are building a list of tuples, where each tuple consists of:\n",
    "\n",
    "#context: A list of words surrounding a target word (2 words before and 2 words after).\n",
    "#target: The current word being predicted by the context.\n",
    "#Loop: The loop iterates through the words list (from index 2 to len(words) - 2), ensuring that the target word has 2 words before and 2 words after it (i.e., a context size of 2).\n",
    "\n",
    "#Appending Data: For each word i in the range, the context is the 2 preceding and 2 succeeding words, and the target is the word at position i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4cc35",
   "metadata": {
    "id": "0601eeea"
   },
   "outputs": [],
   "source": [
    "#Embedding\n",
    "embeddings =  np.random.random_sample((vocab_size, embed_dim))\n",
    "\n",
    "#Initializes a random embedding matrix of shape (vocab_size, embed_dim), where each word in your vocabulary is represented by a random vector of length embed_dim (in this case, 10).\n",
    "\n",
    "#This matrix will be refined during training, where the embeddings (vectors) for words are adjusted to capture semantic relationships based on the context and target pairs you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963acf5e",
   "metadata": {
    "id": "63554686"
   },
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "def linear(m, theta):\n",
    "    w = theta\n",
    "    return m.dot(w)\n",
    "\n",
    "#n short, the linear function calculates the dot product between a matrix m (context word embeddings) \n",
    "# and a vector theta (target word embedding). It essentially measures the similarity between the context and target word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0471b",
   "metadata": {
    "id": "6dbefcbc"
   },
   "outputs": [],
   "source": [
    "# Log softmax + NLLloss = Cross Entropy\n",
    "\n",
    "def log_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.log(e_x / e_x.sum())\n",
    "\n",
    "#The log_softmax function computes the log of the softmax values,\n",
    "#  which is useful for tasks like classification, and it helps in calculating cross-entropy loss efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a2545",
   "metadata": {
    "id": "a7796bee"
   },
   "outputs": [],
   "source": [
    "def NLLLoss(logs, targets):\n",
    "    out = logs[range(len(targets)), targets]\n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "\"\"\"logs: This is the input to the loss function, typically the output of the log_softmax function. It is a matrix where each row corresponds to the log-probabilities of all classes for a given example.\n",
    "\n",
    "targets: This is an array of the true class labels (indices) for each example. Each value in targets is an index corresponding to the correct class for that particular sample.\n",
    "\n",
    "logs[range(len(targets)), targets]: This selects the log-probabilities for the correct classes. range(len(targets)) provides the row indices (for each sample), and targets provides the column indices (for the correct class).\n",
    "\n",
    "out.sum(): This computes the sum of the log-probabilities for the correct class for all samples.\n",
    "\n",
    "-out.sum() / len(out): The negative sum of the log-probabilities is averaged over the number of samples, which gives the mean Negative Log-Likelihood Loss. This is the loss value that is minimized during training.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4bc761",
   "metadata": {
    "id": "640942e6"
   },
   "outputs": [],
   "source": [
    "def log_softmax_crossentropy_with_logits(logits,target):\n",
    "\n",
    "    out = np.zeros_like(logits)\n",
    "    out[np.arange(len(logits)),target] = 1\n",
    "\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "\n",
    "    return (- out + softmax) / logits.shape[0]\n",
    "\n",
    "\n",
    "\"\"\"logits: These are the raw model outputs (before applying softmax), which represent the unnormalized scores for each class.\n",
    "\n",
    "target: This is the array of true class labels (indices) for each sample. Each value corresponds to the correct class for that sample.\n",
    "\n",
    "out = np.zeros_like(logits): Creates a zero matrix of the same shape as logits. This will eventually be used to represent the one-hot encoding of the correct class.\n",
    "\n",
    "out[np.arange(len(logits)), target] = 1: This sets the appropriate element in the out matrix to 1 for each sample's correct class (this is the one-hot encoding of the correct labels).\n",
    "\n",
    "softmax = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True): This computes the softmax function. It exponentiates the logits, then normalizes the values across each sample to get the class probabilities.\n",
    "\n",
    "return (- out + softmax) / logits.shape[0]:\n",
    "\n",
    "- out + softmax: This computes the element-wise difference between the softmax output and the one-hot encoded target labels (which is effectively the cross-entropy loss).\n",
    "/ logits.shape[0]: This averages the loss over all samples\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a82afa",
   "metadata": {
    "id": "943c3a28"
   },
   "outputs": [],
   "source": [
    "#Forward function\n",
    "\n",
    "def forward(context_idxs, theta):\n",
    "    m = embeddings[context_idxs].reshape(1, -1)\n",
    "    n = linear(m, theta)\n",
    "    o = log_softmax(n)\n",
    "\n",
    "    return m, n, o\n",
    "\n",
    "\"\"\"context_idxs: This is a list or array of indices corresponding to the context words for a given target word. Each index refers to a word in the vocabulary.\n",
    "\n",
    "theta: This is the target word's embedding (the weight vector you're trying to learn for the target word).\n",
    "\n",
    "m = embeddings[context_idxs].reshape(1, -1):\n",
    "\n",
    "context_idxs provides the indices of the context words.\n",
    "embeddings[context_idxs] retrieves the corresponding embedding vectors for the context words.\n",
    ".reshape(1, -1) reshapes the context embeddings into a single row (flattening the context vectors into one vector). This is necessary for matrix multiplication in the next step.\n",
    "n = linear(m, theta):\n",
    "\n",
    "This applies a linear transformation (typically a dot product) between the reshaped context embeddings m and the target word's embedding theta. The result n is a raw score (logits) that represents the similarity between the context and the target word.\n",
    "o = log_softmax(n): This applies the log-softmax function to the raw output n, converting it into log-probabilities.\n",
    "\n",
    "return m, n, o: The function returns:\n",
    "\n",
    "m: The reshaped context embeddings.\n",
    "n: The raw scores (logits).\n",
    "o: The log-probabilities after applying the softmax.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23f00757",
   "metadata": {
    "id": "b5085ed8"
   },
   "outputs": [],
   "source": [
    "def backward(preds, theta, target_idxs):\n",
    "    m, n, o = preds\n",
    "\n",
    "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
    "    dw = m.T.dot(dlog)\n",
    "\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e636c86e",
   "metadata": {
    "id": "797b80ce"
   },
   "outputs": [],
   "source": [
    "# Optimize function\n",
    "\n",
    "def optimize(theta, grad, lr=0.03):\n",
    "    theta -= grad * lr\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc556fba",
   "metadata": {
    "id": "07ab0dc7"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "#Genrate training data\n",
    "\n",
    "theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4a52467",
   "metadata": {
    "id": "a4e6971e"
   },
   "outputs": [],
   "source": [
    "epoch_losses = {}\n",
    "\n",
    "for epoch in range(80):\n",
    "\n",
    "    losses =  []\n",
    "\n",
    "    for context, target in data:\n",
    "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
    "        preds = forward(context_idxs, theta)\n",
    "\n",
    "        target_idxs = np.array([word_to_ix[target]])\n",
    "        loss = NLLLoss(preds[-1], target_idxs)\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        grad = backward(preds, theta, target_idxs)\n",
    "        theta = optimize(theta, grad, lr=0.03)\n",
    "\n",
    "\n",
    "    epoch_losses[epoch] = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4badda9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "0d0ba2b3",
    "outputId": "2452cb14-4e5b-40d9-cec4-3a311c718fcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Losses')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEmCAYAAACDLjAiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAr4UlEQVR4nO3dd3gc5bn38e+tXi1ZltwrmGYbbGPZ2JhqE0oKBAKhHCAQSgLhnORNTniTnHNy4D2pJ4RACgmEFgglhBYgBDDFFAM2Mhjjbtxwt1wkWZLV7/ePGYEi5CJb2lnt/j7XNdfuzszO3Ctbvx0988wz5u6IiEhiS4m6ABER6X4KexGRJKCwFxFJAgp7EZEkoLAXEUkCCnsRkSSgsJeEZGY3mJmb2UlR1yISDxT20qEwKPc2nRR1nd3FzP5uZtvNLC18vTr8zMMjLk1kv6RFXYDEvRv3sGx1rIqIJTPLB6YDf3H3pqjrEekKCnvZI3e/IeoaIvBZIBN4IupCRLqKmnGkS7RtIzezr5jZe2a2y8y2mNndZtZ/N+87xMzuM7P1ZtZgZhvC14fsZv1UM/u6mc0ys8pwHx+a2Z17eM+5ZjbHzGrDppmHzWzQHj7OOUAt8HynfxDB/lLCGt8xs2ozqwmfX2Nmn/qdM7PjzexpM1tnZvVmtsnM3jaz/263Xj8zu8nMlobbrAif32tmB3Ww3dPM7Fkz2xpud4WZ/cLMCjtY9ygzeyhsrqo3s3Ize9fMbjGz9P35OUh8MY2NIx0xMwdwd9vH9W8A/ht4CjgV+AuwETgunFYBx7h7eZv3TAReBPLD9y0CDgfOAqqB6e5e1mb9DODvwCnA2vB5FTA8nPcdd7+3XT1/Bc4Mt78GOAY4HlgCjHP3+nafIxMoB2a4+5fazF8NDANGuPvqvfwsHgAuCmt8HHDg7PD9D7r7v7RZ9/Q2n+MpYD1QBBwBHO7u/cL1coD5wMHAjPC5hducDlzi7s+02e4PCZrgtgPPAFuAowj+bRYBU9y9Klz3KGB2WOdTBP9WvYCRwMlAkbtX7+kzSw/g7po0fWoi+MV34IbdTN9rt/4N4foNwPh2y34VLrurzTwDFofz/6Xd+ueH85cAKW3m/4RPAimz3XsygZIO6qkCjmy37oPhsi938Lk/Fy67uN381eH84Xv5uV0YrvcukNdmfi5QFi67qM38x8J5YzvYVnGb518I1/tVB+tlAPltXp8crvsmUNhu3cvabwf4ZTjvrA623bvtv4GmnjtFXoCm+JzahP3upop267eG610dbKsAqAB2tYY0MLU1kHaz/9fD5SeEr1PDbdQCA/eh/tZ6ftTBstYwvKmDZXcSfGEVtpu/r2E/I1zv1A6WTQ+XvdxmXmvYH7qX7baG/U/24bM/Ea47ejfL3wO2tHn9y93VrClxJp2glT3yfWzGaePVDrZRaWbzgBMJmifmAUeHi1/ezXZeJmj+GQ+8RtC8UwDMdvcNnainrIN5a8PH3m1nmlkqQZPPTHev6MQ+2joaaAFmdrDsVaCZ4DO1eoDgHMFsM/sL8Aowy93XdfDe9cD3zOxo4FlgFjDP3ZvbrTsFaATOM7PzOqgjAygxsz7uvo2gye2bwJNm9ihB09osd1+xj59ZegCFvXS1zbuZvyl8LGj3uHE367fOL2z3uL6T9VR0MK+1O2Vqu/nHASUE7ez7qwDY7u4N7Re4e5OZbQX6tpn3uJl9HvgO8FXgawBmNhf4vrvPCNerMrPJBO3wZwKnhZvYama3EfwF0xjO60Pwu/1PJ3g7kAdsc/c5ZnY88B/AucAlYQ1LgRvd/aHO/hAk/qg3jnS1fruZ39obp7LdY4e9dIAB7darCB/31IvmQJ1N0JzxtwPYRiVQ1FEPlvACrWKC8wgfc/e/u/s0gr80phOc4xgNPGNmo9qst87dryD4shgD/BuwDfhhOLWtYYe7216mNW22/Za7fz6sYSrwPwT/lg+a2SkH8POQOKGwl652YvsZZlYAjAPqCE7KQtBuDHDSbrbTOv/d8HEJQeAfZWYDD7zMDn0ReNvdd/fXxr54j+D36oQOlp1A8NfEux0sw91r3P1ld/82wcnoDOCMDtZzd1/o7r8BPtOm9lZvA73NbHRni3f3end/091/SPBlAkHvKOnhFPbS1S4xs/Ht5t1A0LzxkH/S1XEWsBQ4zszObbty+PoEYBnwBkDYLn0bkA38Iewi2fY9GWZWsr9Fm9kEgm6MB3oh1d3h40/D7pKt288Bfha+vKvN/Olmlt3Bdlr/QqoN1xuzm6Ea/mm90K/Cxz929MVoZrlhk1Dr6+PDL+R92bb0UGqzlz0K+6vvzpPuPq/dvH8As8zsEf65n/1q4HutK7m7m9lXCHqv/MXM/kZw9H4YwVHqTuBSd29ps+0bCfrJfwFYZmbPhOsNIeg//l3g3v34mBA04cDew/4mM9tdn/MfuvuDZnYW8GVgoZk9SdA09EVgBPCIuz/Q5j2/BIab2UyCn1EDMAGYRnBdwMPheqcAN5vZmwQ/py3AYIKj7hbgF60bdPeXzOx7wE+B5Wb2LEHf+TyCL7QTCb5ETw/f8h3g1LCGlQTXOIwm+KtiB3DHXn4m0hNE3R1IU3xO7L3rpQOXtVn/hnDeSQR9uecRdLUsB+4BBuxmP4cB9xN8MTSGj38GDtvN+mnAdcAcglCqAZYTBNLIjurpYBvDw2X3tpm3EJi/h5/H6n34eYwL100BriXoCVQbTnOBb9CuzzrBl8JD4WeoJmjPXwD8mH++buAI4OZwm+VAfVjTo8Cxu6n5OOARYAPBl0h5+O9yM1DaZr1Tw3+jRQTt/TUEf3X9GhgW9f9FTV0z6Qpa6RJtrlg92d1nRltN55jZoQTh9v/cfW89WER6JLXZi+x7E45Ij6Wwl6Tn7j/3oCvivKhrEekuCnsRkSSgNnsRkSSgI3sRkSSgsBcRSQIKexGRJKCwFxFJAgp7EZEkoLAXEUkCCnsRkSSgsBcRSQJxO8RxcXGxDx8+POoyRER6jLlz52519w7v6xC3YT98+HDKyjq6V7SIiHTEzNbsbpmacUREkoDCXkQkCSjsRUSSgMJeRCQJKOxFRJKAwl5EJAko7EVEkkBChX1Li/Pbl5fz6rLyqEsREYkrCRX2KSnGHa+t5KXFm6MuRUQkriRU2AMMLMxmQ8WuqMsQEYkrCRn26yvqoi5DRCSuJGDYZ7GxUkf2IiJtJWDYZ1NR20hNfVPUpYiIxI2EC/tBhdkAOroXEWkj4cJ+YBj2arcXEflEwoX9gIIsAPXIERFpI+HCvl+vLFIMNirsRUQ+lnBhn56aQr9eWWrGERFpI+HCHnRhlYhIe4kb9uqNIyLyscQM+4IsNlbU0dLiUZciIhIXEjPsC7NpaG5hW01D1KWIiMSFhA17UPdLEZFWCRr26msvItJWQob9oI+volXYi4hADMPezArN7FEzW2Jmi81sSnftqyA7nZyMVDaor72ICABpMdzXrcBz7n6umWUAOd21IzNjQIGGOhYRaRWTsDezXsAJwGUA7t4AdGtXGV1YJSLyiVg14xwElAP3mNl7ZnanmeW2X8nMrjazMjMrKy8/sJuGD9Idq0REPharsE8DjgZ+7+7jgRrge+1Xcvc73L3U3UtLSkoOaIcDC7PZWl1PXWPzAW1HRCQRxCrs1wHr3H12+PpRgvDvNq197TdV6uheRCQmYe/um4C1ZnZYOGs6sKg79zmwdVx7naQVEYlpb5x/BR4Ie+KsBC7vzp19chWtjuxFRGIW9u4+DyiN1f76645VIiIfS8graAGy0lMpzstU2IuIkMBhDzCoMIsNOkErIpLYYa8Lq0REAgkd9gMKgrB3101MRCS5JXTYDyzMorahmcpdjVGXIiISqYQOew11LCISSOiwb+1rv1F97UUkySV02A8pCkZRXlFeHXElIiLRSuiwL8rNYHifHMrW7Ii6FBGRSCV02ANMGFbE3DU71CNHRJJawof9xOG92V7TwMqtNVGXIiISmYQP+9LhRQCUrd4ecSUiItFJ+LA/uCSX3jnpvLNa7fYikrwSPuzNjNLhRTqyF5GklvBhD0G7/epttWzZqf72IpKckiLsW9vt56opR0SSVFKE/ZiBBWSmpajdXkSSVlKEfUZaCuOGFFK2Ru32IpKckiLsAUqH92bhhipqG5qiLkVEJOaSKOyLaG5x5n1UEXUpIiIxlzRhf/TQ3pihdnsRSUpJE/YF2ekc1i9f7fYikpTSYrUjM1sN7ASagSZ3L43VvltNHF7E4++uo6m5hbTUpPmeExGJ+ZH9ye4+Loqgh+AkbU1DM4s2VkWxexGRyCTV4e3UkcWkGLy4eEvUpYiIxFQsw96BF8xsrpld3dEKZna1mZWZWVl5eXmXF1Ccl0npsCJeWLipy7ctIhLPYhn2U939aOAM4BtmdkL7Fdz9DncvdffSkpKSbini1NH9WLJpJ2u2aXx7EUkeMQt7d98QPm4BngAmxWrfbZ02uj8Az+voXkSSSEzC3sxyzSy/9TlwKrAgFvtub0hRDqMG9OK5BQp7EUkesTqy7we8YWbvA3OAv7v7czHa96ecNro/735UwZYqDXksIskhJmHv7ivdfWw4jXb3H8div7tz+pigKeeFRZujLENEJGaSqutlq0P75TG8T47a7UUkaSRl2JsZp43uz1srtlG5qzHqckREul1Shj3AqaP709TivLxETTkikviSNuzHDymkb34mzy9Q2ItI4kvasE9JMT4zqh+vLivXDU1EJOElbdgDnDl2ILsam3nm/Y1RlyIi0q2SOuwnjSji0H553P/2mqhLERHpVkkd9mbGxZOH8cH6St5fWxF1OSIi3Sapwx7g7PGDyMlI1dG9iCS0pA/7/Kx0vjh+EE+/v4GK2oaoyxER6RZJH/YAFx8zjPqmFh6duy7qUkREuoXCHhg1sBcThvXmgdkf0dLiUZcjItLlFPahiycPZdXWGmat2Bp1KSIiXU5hHzpjzACKcjO4/y2dqBWRxKOwD2Wlp3LBxCHMWLyZJZuqoi5HRKRLKezbuPqEg8jPTONn/1gSdSkiIl1KYd9GYU4G100bycyl5cz6UG33IpI4FPbtXDplOIMKs/nJs4vVM0dEEobCvp2s9FS+e9phLNxQxd/eXx91OSIiXUJh34Ezxw5kzKBe3PT8Muoam6MuR0TkgCnsO5CSYvzgjCNYX7GLP725OupyREQOmMJ+N44dWcz0w/vyqxeXsWzzzqjLERE5IDENezNLNbP3zOyZWO53f/30S0eSl5nOtQ+8S0297mYlIj1XrI/svwksjvE+91vf/Cx+fcE4VpZX819PLsBdvXNEpGeKWdib2WDgc8CdsdpnVzh2ZDHfnH4oj7+3nkfK1kZdjojIfonlkf0twPVAy+5WMLOrzazMzMrKy8tjVtjeXDdtJMeNLOaHf1vIog0aSkFEep59DnszKzGzvPB5qpldbmaXmtlet2Fmnwe2uPvcPa3n7ne4e6m7l5aUlOxrad0uNcW45YJxFGSnc+Wf3mF9xa6oSxIR6ZTOHNk/AxwSPv8x8O/At4Ff7sN7pwJnmtlq4GFgmpn9uRP7jlxxXib3XD6RnfVNXHLnbLZW10ddkojIPutM2B8KzAufXwycAUwDLtjbG939++4+2N2Hh+u/7O4Xd67U6I0eWMA9l01kQ+UuLr1rDpW7GqMuSURkn3Qm7JuBDDM7Eqh094+ACiCvOwqLV6XDi7j9klKWb9nJlX96h10NusJWROJfZ8L+H8AjwO8JmmIARgGdGkDG3We6++c78554c+KhJdxy/njmrtnB1feXaUgFEYl7nQn7K4G/A3cBPw3nFQM3dHFNPcLnjhrAz790FK8v38p1D75LQ9NuOxmJiEQubV9XdPd64I6w900/YKO7z+yuwnqC80qHUNfUwn89uYBv/eU9fn3BeNJSNQKFiMSfznS9LDSzB4E64MNw3plm9qPuKq4nuGTyMP7zc0fw7Aeb+O6j82nWGPgiEoc6cxj6B6ASGAY0hPPeAs7v6qJ6miuPP4h/P/VQnnhvPf+pYRVEJA7tczMOMB0Y6O6NZuYA7l5uZn27p7Se5bpph1DT0MzvZ66gV1Ya3zvjcMws6rJERIDOhX0lwQnZja0zzGxo29fJ7vrTDqO6ronbX1tJflYa1007ZO9vEhGJgc6E/Z3AY2b2H0CKmU0BfkLQvCOAmXHjmaOprm/ipheWkZeZxmVTR0RdlohIp8L+5wQnZ38HpAN3A7cDt3ZDXT1WSorxi3OPorq+iRueXkR+VjpfmjA46rJEJMnt8wlaD9zi7qPcPdfdjwhf62xkO2mpKfzmwvFMHdmH6x+bzwsLN0Vdkogkuc50vTzZzEaEz/ub2Z/M7G4z69995fVcWemp3H5JKUcOKuC6B9/jzQ+3Rl2SiCSxznS9vI1gfByAmwmachy4o6uLShR5mWnce/lERhTncuV9Zbz30Y6oSxKRJNWZsB/k7h+ZWRpwGnA1cA1wbLdUliAKczK4/4pJFOdlctk977Bkk25+IiKx15mwrzKzfsCJwCJ3rw7np3d9WYmlb68sHrjyGLLSU7j4zjms2loTdUkikmQ6E/a/Ad4BHiDokQPBTUmWdHVRiWhIUQ4PXHkMLe5cfOds3e1KRGKqM71xfg6cAkx199YhjtcTjIYp+2Bk33zu++okquoaufjO2WzZWRd1SSKSJDo1RKO7L3P3FRD0zgH6u/sH3VJZghozqIB7L5/Ipso6LrlzDjtqGvb+JhGRA9SZrpevmtnU8Pn/JbiByUNm9oPuKi5RTRhWxJ1fKWXVthouuXu2bm8oIt2uM0f2Y4C3w+dXAScBk4Gvd3FNSWHqyGJuv3gCSzft5LJ75lBd3xR1SSKSwDoT9imAm9nBgLn7YndfC/TuntIS38mH9+U3Fx7N/HWVfPVe3c9WRLpPZ8L+DeC3wE3AEwBh8OvS0ANw+pj+/Or8cZSt3s6V972j+9mKSLfoTNhfBlQA8/nkvrOHo4HQDtiZYwfyv+eO5c0V27jqPt3AXES6XmfuQbsN+EG7eX/v8oqS1LkTBtPS4lz/2Hy+/ue53H7JBDLTUqMuS0QSRGd646Sb2Y1mttLM6sLHG80sYx/em2Vmc8zsfTNbaGY3HljZienLE4fws3OOZObScq7587vUN+kIX0S6Rmeacf6X4KKqrwNjw8dpBOPc7009MM3dxwLjgNPNbHLnSk0OF0wayo/PHsPLS7ZwrQJfRLpIZ8L+POBMd3/B3Ze6+wvA2cCX9/bGcCz8tmPptI6YKR34l2OG8aMvjuGlJVv4+v1z1YYvIgesM2G/u7tn79Ndtc0s1czmAVuAGe4+uxP7TjoXTx7GT84+kleWlvM1Bb6IHKDOhP1fgafN7DQzO8LMTgeeBB7Zlze7e7O7jwMGA5PMbEz7dczsajMrM7Oy8vLyTpSWmC46Zig/O+dIXlterl46InJAOhP21wMvEox4OZdgFMxXgE4N7uLuFcBM4PQOlt3h7qXuXlpSUtKZzSasCyYN5efnHMUbH27lij+9Q22DrrQVkc7rzKiXDe7+Q3cf6e457n4I8GPgO3t7r5mVmFlh+Dyb4ESvhkbeR1+eOISbzh3LWyu2cdnd77CzTmPpiEjndGrUyw44+9ZmPwB4xczmE4yJP8PdnznAfSeVL00YzK0XjGfuRzu45K45GjxNRDplny+q2oO99qpx9/nA+C7YV1L7wtiBZKSlcN2D73LRH9/m/iuOoSh3r5c5iIjsPezNbNoeFitpYuy00f2549JSvnb/XM6//S3+fOUx9OuVFXVZIhLnzH3PB+ZmtmpvG3H3EV1WUai0tNTLysq6erMJ480VW7nqT2X0ycvkgSuPYUhRTtQliUjEzGyuu5d2tGyvbfbuPmJvU9eXLHtz7MHFPHDVZCp3NXLeH97iwy3Ve3+TiCStAz1BKxEaN6SQv3xtMk0tzpdvf4sF6yujLklE4pTCvoc7vH8v/vr1KWSnp3LBHW/z1optUZckInFIYZ8ARhTn8ug1UxhQkMVX7pnD8ws3RV2SiMQZhX2CGFCQzSNfm8KoAb245s9zeaRsbdQliUgcUdgnkN65GTxw5TFMHVnM9Y/O57aZH7K33lYikhwU9gkmNzONu74yMbjV4XNLueGphTS3KPBFkl1XXEErcSYjLYVbzh9Hv16Z/PH1VZRX13Pzl8eRla7bHIokK4V9gkpJMf7jc6Pom5/Fj59dzNbqOfzxklIKctKjLk1EIqBmnAR31QkHcesF45j3UQXn/H4Wa7fXRl2SiERAYZ8Ezho3iPuumET5znrOvm0W89dVRF2SiMSYwj5JTD6oD49feyxZ6amcf/vbzFi0OeqSRCSGFPZJZGTffJ64diqH9svj6vvLuPP1leqaKZIkFPZJpiQ/k4evnsLpo/vzo78v5gdPLKCxuSXqskSkmynsk1B2Riq/u+horj3pYB6a8xGX3TOHylrd+UokkSnsk1RKinH96Ydz03ljmbNqO2ffNosV5RomWSRRKeyT3LkTBvNgOC7+F387i1eWbom6JBHpBgp7YeLwIp761+MYUpTDV+99h9tfXaETtyIJRmEvAAwqzObRa6bw2TED+Ok/lvDNh+dR29AUdVki0kUU9vKxnIw0fnvReL572mE8PX8D59z2Jqu31kRdloh0gZiEvZkNMbNXzGyxmS00s2/GYr/SeWbGN04eyb2XT2JTVR1f+O0bvLxEF2CJ9HSxOrJvAr7j7kcAk4FvmNmoGO1b9sOJh5bw9HXHMaR3Dlf8qYxfvrBUQyWL9GAxCXt33+ju74bPdwKLgUGx2LfsvyFFOTx2zbF86ejB/OblD7nkrtmU76yPuiwR2Q8xb7M3s+HAeGB2rPctnZedkcpN543lf889irlrdvC5X7/O7JW6qblITxPTsDezPOAx4FvuXtXB8qvNrMzMysrLy2NZmuzFl0uH8OQ3ppKXmcaFf3ybW19crmYdkR7EYtWf2szSgWeA59395r2tX1pa6mVlZd1fmHTKzrpG/vPJBfxt3gaOGVHELReMY0BBdtRliQhgZnPdvbSjZbHqjWPAXcDifQl6iV/5Wenccv44bjpvLB+sr+SMW1/XcMkiPUCsmnGmApcA08xsXjh9Nkb7li5mZpw7YTBP/+txDCrM5qr7yvj+4x/oIiyROBaTe9C6+xuAxWJfEjsHl+Tx+LXHcvMLy7jj9ZW8tWIrvzp/HOOH9o66NBFpR1fQygHJTEvl+589goeumkxjs3PuH97i5hnLaGjSGPki8URhL11i8kF9+Me3juessQP59UvLOet3s1i04VMdrkQkIgp76TK9stK5+fxx3H7JBMp31nPmb9/glheX6U5YInFAYS9d7rTR/Znxf07gc0cN4JYXl3Pmb2cxf11F1GWJJDWFvXSL3rkZ3HrBeO64ZALba+r54u9m8T/PLFKPHZGIKOylW506uj8zvn0iFx0zlLveWMVnbn6NV5boblgisaawl27XKyudH33xSP769SlkZ6Ry+b3v8LX7y1hfsSvq0kSShsJeYmbi8CKe/bfjuf70w3ht2VZO+eWr3DbzQ3XTFIkBhb3EVEZaCteeNJIZ3z6B4w8p5n+fW8ppt7zGS4s36763It1IYS+RGNw7hzsuLeWeyydiBlf8qYxL757D8s07oy5NJCEp7CVSJx/Wl+e/dQL/9flRzFtbwem3vs5/PvmBbpIi0sUU9hK59NQUrjhuBDP//SQumjSUh+es5aRfvMKvX1qurpoiXSRm49l3lsazT14ry6v5xfNL+ceCTZTkZ/Kv00Zy/sQhZKalRl2aSFyLfDx7kc44qCSP3188gceumcKIPrn88G8LmXbTqzzyzlqaNPSCyH5R2EvcmjCsiL98bTL3fXUSxXkZXP/YfD7zq9d4dO46hb5IJ6kZR3oEd2fGos3c8uJyFm2sYmhRDteedDDnHD2YjDQds4jAnptxFPbSo7g7Ly3ewq9fXs78dZUMLMjiq8eN4MJJQ8nNjMm9eETilsJeEo67M3NZOb+fuYI5q7ZTkJ3OpVOGcemU4ZTkZ0ZdnkgkFPaS0N79aAd/mLmCGYs3k56SwlnjBnL51BGMGtgr6tJEYkphL0lhRXk198xaxWNz17OrsZkpB/XhsqnDmX54X9JS1a4viU9hL0mloraBh99Zy31vrmZDZR0DCrK4cNJQLpg4hL69sqIuT6TbKOwlKTU1t/Dyki3c//YaXl++lbQU45Qj+nH+pCGccEgJqSkWdYkiXWpPYR+T7gtmdjfweWCLu4+JxT5F0lJTOHV0f04d3Z9VW2t4cPYaHnt3Pc8t3MSAgizOmzCYL00YzLA+uVGXKtLtYnJkb2YnANXAffsa9jqyl+7Q0NTCS4s38/A7a3lteTnuUDqsN1+aMJjPHjmAguz0qEsU2W9x0YxjZsOBZxT2Ei82Vu7iiffW89jcdaworyEjLYWTDyvhC2MHMv3wfmRnaCwe6Vkib8YRiUcDCrK59qSRXHPiwXywvpLH313P3z/YyPMLN5Obkcopo/rx2SMHcOKhJWSlK/ilZ4urI3szuxq4GmDo0KET1qxZE5PaRFo1tzizV27jqfc38NzCTVTUNpKTkcrJh/fljDH9OfHQEvKz1NQj8UnNOCL7obG5hdkrt/Psgo08v2AT22oaSE81Jh/Uh1NH9WP6Ef0YWJgddZkiH1PYixyg5hZn7podvLh4MzMWbWbV1hoADu+fz8mH92Xa4X0ZP6RQF29JpCIPezN7CDgJKAY2A//t7nft6T0Ke4lX7s6K8mpeXrKFl5dsoWz1DppanPysNKYeXMzxhxZzwiElDCnKibpUSTKRh/3+UNhLT1FV18gby7fy2rJyXltWzobKOgCGFuVw7MF9OHZkMcce3IfiPA3QJt1LYS8SI+7Oyq01vLasnDdXbOPtldvYWRfcR3dk3zyOGVHEpBFFTD6oD/00dIN0MYW9SESamltYuKGKN1dsY/aqbZSt3kF1fRD+Q4qyKR1WROnw3pQOK2Jk3zwN4SAHRGEvEieamltYtLGKOau2U7Z6B2VrdrC1uh6AvMw0xg4pYPyQ3owbUsjYIYUam186RWEvEqfcnY+21zJ3zQ7e+6iC99buYPHGnTS3BL+XAwuyOGpwIUcNKWDMwALGDCqgKDcj4qolXukKWpE4ZWYM65PLsD65nHP0YABqG5pYuKGK99dW8P66Suavq+C5hZs+fs/AgixGDSxg1MBejBoQTIN7Z5OiJiDZA4W9SJzJyUhj4vAiJg4v+nheZW0jCzdUsmBDJR+sr2LxxipeXrKZ8A8AcjNSOax/Pof178URA/I5pG8+h/bLo496AElIzTgiPdSuhmaWbt7Jog1VLN1UxZJNO1myaSeVuxo/XqdPbgaH9MtjZN88RpbkcXDf4Hn/XlmY6S+BRKNmHJEElJ2RyrghhYwbUvjxPHdnc1U9yzbvZNnmnSzfXM2yLTv527wNH3cBBcjJSGVEcS4HleQxojiXEcU5DO+Ty4jiXApzdE4gESnsRRKImdG/IIv+BVmccGjJx/PdnfLqej7cUs2K8hpWllezsryGeWt38Mz8DbT9A78gO51hfXIYWhR8AQwtymFIUQ5DirIZUJCt7qE9lMJeJAmYGX3zs+ibn8WxBxf/07L6pmbWbq9l1dZaVm+tYfW2Gj7aXsv8dZX8Y8Gmj3sGAaSlGAMLsxncO5gGFeYwqHc2g8J5/XplkZGm8YHikcJeJMllpqUysm8+I/vmf2pZY3MLGyvq+Gh7LWt31PLR9lrW79jF2h21vLK0nPKd9f+0vhmU5GUyoDCbgQVZDCjIZkD4l8aAgiz69cqib69MMtN0f4BYU9iLyG6lp6YwtE8OQ/t0PKhbXWMzGyvr2FCxi/U7drG+YhcbK3exsbKOpZt3MnNpObsamz/1vqLcDPr1yqJfr0z65mcGXwL5mZTkZ1GSnxk+z9RNY7qQwl5E9ltWemp4grfjm7a7O1V1TWyqrGNj5S42V9WxuaqeTVV1bK6sY8vOehZtqGJrdT0tHXQMzM9Mozg/k+K8DEryM+mTm0lxXiZ98jIozsugKDeTotzgea+sdF1rsAcKexHpNmZGQXY6BdnpHNb/081ErZpbnG3V9WzZWU95dT3lVeHjznq2VgfT0k072V6zjR21jR1uIzXF6J2TTlFuBr1zMuiTl0FhTgZFORn0zs2gd046hTnpFOYEywuz0+mVnZ40J5wV9iISudQUo2+vLPruw0igjc0t7KhpoLy6nu01DWyvaWBrdQPba+rZXtMYPjawbHM1O2oa2FHb0OFfDRCcY+iVFXwJtH4ptZ16tX2elU6v7LTwMZ38rDTSe9DNahT2ItKjpKem7PMXA0BLi1NV10hFbSM7ahs+fqzc1ciO2kYqaxuo2NVI5a5gnfU7dlGxq5GqXY007e5bIpSVnkJ+Vjq9stLIzwq+AHqFj3mZaeSFj8Hr9PB1KrmZ4fLMNHIzY/OlobAXkYSWkmIU5gRNOsPp+NxCR9yd2oZmquqCL4KddU1UhV8KVa2v6xqp2tXEzvrg9c66JjZU7KK6vonquiZqGj59crojGWkp5GYEXwIDC7J55OtT9vfj7pbCXkSkA2ZGbnjkPaBg/24s39ziVNc1Ud3QRE198GVQXR88r277PFxeW99MZnr3HOUr7EVEuklqilGQk05BTnrUpdBzzi6IiMh+U9iLiCQBhb2ISBKIWdib2elmttTMPjSz78VqvyIiEqOwN7NU4HfAGcAo4EIzGxWLfYuISOyO7CcBH7r7SndvAB4GzorRvkVEkl6swn4QsLbN63XhPBERiYFYhX1HIw196jpkM7vazMrMrKy8vDwGZYmIJIdYXVS1DhjS5vVgYEP7ldz9DuAOADMrN7M1+7m/YmDrfr63O8VrXRC/tcVrXRC/tcVrXRC/tcVrXdC52obtboG573mgn65gZmnAMmA6sB54B7jI3Rd20/7KdneH9SjFa10Qv7XFa10Qv7XFa10Qv7XFa13QdbXF5Mje3ZvM7DrgeSAVuLu7gl5ERD4tZmPjuPuzwLOx2p+IiHwiUa+gvSPqAnYjXuuC+K0tXuuC+K0tXuuC+K0tXuuCLqotJm32IiISrUQ9shcRkTYSKuzjafwdM7vbzLaY2YI284rMbIaZLQ8fe0dQ1xAze8XMFpvZQjP7ZhzVlmVmc8zs/bC2G+OltrCOVDN7z8yeibO6VpvZB2Y2z8zK4qU2Mys0s0fNbEn4/21KnNR1WPizap2qzOxbcVLb/wn/7y8ws4fC34kuqSthwj4Ox9+5Fzi93bzvAS+5+yHAS+HrWGsCvuPuRwCTgW+EP6d4qK0emObuY4FxwOlmNjlOagP4JrC4zet4qQvgZHcf16aLXjzUdivwnLsfDowl+NlFXpe7Lw1/VuOACUAt8ETUtZnZIODfgFJ3H0PQc/GCLqvL3RNiAqYAz7d5/X3g+xHXNBxY0Ob1UmBA+HwAsDQOfm5/Az4Tb7UBOcC7wDHxUBvBhYAvAdOAZ+Lp3xNYDRS3mxdpbUAvYBXhecF4qauDOk8FZsVDbXwyrEwRQU/JZ8L6uqSuhDmyp2eMv9PP3TcChI99oyzGzIYD44HZxEltYVPJPGALMMPd46W2W4DrgZY28+KhLgiGHnnBzOaa2dVxUttBQDlwT9j0daeZ5cZBXe1dADwUPo+0NndfD9wEfARsBCrd/YWuqiuRwn6fxt+RgJnlAY8B33L3qqjraeXuzR78eT0YmGRmYyIuCTP7PLDF3edGXctuTHX3owmaML9hZidEXRDBkenRwO/dfTxQQ7TNXJ9iZhnAmcBfo64FIGyLPwsYAQwEcs3s4q7afiKF/T6NvxOxzWY2ACB83BJFEWaWThD0D7j74/FUWyt3rwBmEpz3iLq2qcCZZraaYHjuaWb25zioCwB33xA+biFoe54UB7WtA9aFf5kBPEoQ/lHX1dYZwLvuvjl8HXVtpwCr3L3c3RuBx4Fju6quRAr7d4BDzGxE+I19AfBUxDW19xTwlfD5Vwjay2PKzAy4C1js7jfHWW0lZlYYPs8m+M+/JOra3P377j7Y3YcT/L962d0vjrouADPLNbP81ucEbbwLoq7N3TcBa83ssHDWdGBR1HW1cyGfNOFA9LV9BEw2s5zw93Q6wUntrqkrypMj3XCC47MEA66tAP4j4loeImh3ayQ4yrkC6ENwkm95+FgUQV3HETRvzQfmhdNn46S2o4D3wtoWAD8M50deW5saT+KTE7SR10XQNv5+OC1s/X8fJ7WNA8rCf88ngd7xUFdYWw6wDShoMy/y2oAbCQ5wFgD3A5ldVZeuoBURSQKJ1IwjIiK7obAXEUkCCnsRkSSgsBcRSQIKexGRJKCwF+liZuZmNjLqOkTaUthLwguHAN5lZtVtpt9GXZdILMXsHrQiEfuCu78YdREiUdGRvSQtM7vMzGaZ2W/MrDK8ycb0NssHmtlTZrbdghviXNVmWaqZ/cDMVpjZznDEybZjM50S3mxih5n9Lrz8HTMbaWavhvvbamZ/ieFHliSmI3tJdscQDNJVDJwDPG5mI9x9O8GQFwsJRiA8HJhhZivd/SXg2wRjq7QO0XEUwU0wWn0emEgwrvtc4GngOeB/gBeAk4EMoBSRGNBwCZLwwtEqiwnu0tXquwTjFv0EGOThL4KZzQF+QzDi5mqg0N13hst+SnATicvMbClwvbt/alAqM3PgeHd/I3z9CMHoij8zs/uAOuD/ufu6bvi4Ih1SM44kiy+6e2Gb6Y/h/PX+z0c8awiO5AcC21uDvs2y1hviDCEYcG93NrV5Xgvkhc+vJ7j3wpzwXqNf3c/PI9IpCntJdoNa29NDQwnug7ABKGodPrjNsvXh87XAwZ3dmbtvcver3H0g8DXgNnXTlFhQ2Euy6wv8m5mlm9l5wBHAs+6+FngT+KmZZZnZUQTDVD8Qvu9O4H/M7BALHGVmffa2MzM7z8wGhy93EAw33dzVH0qkPZ2glWTxtJm1DdUZBDeBmA0cAmwFNgPnuvu2cJ0LgT8QHOXvAP7b3WeEy24mGGv8BYLzAUuAs/ehjonALWZWEO7vm+6+6kA+mMi+0AlaSVpmdhlwpbsfF3UtIt1NzTgiIklAYS8ikgTUjCMikgR0ZC8ikgQU9iIiSUBhLyKSBBT2IiJJQGEvIpIEFPYiIkng/wNNH4m65oqh6AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze\n",
    "# Plot loss/epoch\n",
    "\n",
    "ix = np.arange(0,80)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Epoch/Losses', fontsize=20)\n",
    "plt.plot(ix,[epoch_losses[i][0] for i in ix])\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Losses', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2af50e7",
   "metadata": {
    "id": "9b63bfb4"
   },
   "outputs": [],
   "source": [
    "# Predict function\n",
    "\n",
    "def predict(words):\n",
    "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
    "    preds = forward(context_idxs, theta)\n",
    "    word = ix_to_word[np.argmax(preds[-1])]\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74861400",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "8237536c",
    "outputId": "a0a36b9a-a4c2-4ace-e73e-79ba81563d01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'about'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (['we', 'are', 'to', 'study'], 'about')\n",
    "predict(['we', 'are', 'to', 'study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c929e87",
   "metadata": {
    "id": "17af78a5"
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "def accuracy():\n",
    "    wrong = 0\n",
    "\n",
    "    for context, target in data:\n",
    "        if(predict(context) != target):\n",
    "            wrong += 1\n",
    "\n",
    "    return (1 - (wrong / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef3ee34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22ed7c5d",
    "outputId": "b3c2eddf-2ddc-45df-bcee-bac60303d4ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86c2bb83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "f213472d",
    "outputId": "b6129190-c7ba-4a84-e334-2f5a78460e2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(['processes', 'manipulate', 'things', 'study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112b830",
   "metadata": {
    "id": "45ec1ca1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
